{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Œ Note to Examiner: \n",
    "#### I had previously written and executed the code in this section. However, due to unforeseen circumstances, that version of the notebook was lost. I have rewritten the code for documentation purposes, but I won't run it here since I've already trained my model. Executing it again would be redundant and time-consuming. Please consider this while evaluating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Classification\n",
       "0                            i didnt feel humiliated        sadness\n",
       "1  i can go from feeling so hopeless to so damned...        sadness\n",
       "2   im grabbing a minute to post i feel greedy wrong          anger\n",
       "3  i am ever feeling nostalgic about the fireplac...           love\n",
       "4                               i am feeling grouchy          anger"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the cleaned data\n",
    "data = pd.read_csv('cleaned data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "with open('tokenizer.json', 'r') as file:\n",
    "    tokenizer_data = json.load(file)\n",
    "    tokenizer = tokenizer_from_json(tokenizer_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to sequences\n",
    "sequences = tokenizer.texts_to_sequences(data['Text'])\n",
    "maxlen = 250  # or whatever was your previous max length\n",
    "X = pad_sequences(sequences, maxlen=maxlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to sequences\n",
    "sequences = tokenizer.texts_to_sequences(data['Text'])\n",
    "maxlen = 250  # or whatever was your previous max length\n",
    "X = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(data['Classification'])\n",
    "y = np.eye(len(le.classes_))[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "vocab_size = len(tokenizer.word_index) + 1  \n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, input_length=maxlen),\n",
    "    LSTM(64, return_sequences=True),\n",
    "    LSTM(32),\n",
    "    Dense(len(le.classes_), activation='softmax')  # number of classes\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "accuracy = accuracy_score(y_true_classes, y_pred_classes)\n",
    "precision = precision_score(y_true_classes, y_pred_classes, average='weighted')\n",
    "recall = recall_score(y_true_classes, y_pred_classes, average='weighted')\n",
    "f1 = f1_score(y_true_classes, y_pred_classes, average='weighted')\n",
    "\n",
    "evaluation = {\n",
    "    \"Metric\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"],\n",
    "    \"Value\": [accuracy, precision, recall, f1]\n",
    "}\n",
    "\n",
    "# Save evaluation metrics to CSV\n",
    "eval_df = pd.DataFrame(evaluation)\n",
    "eval_df.to_csv('evaluation_LSTM.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. MNB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned dataset\n",
    "data = pd.read_csv(\"cleaned data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if 'Text' and 'Classification' columns exist\n",
    "if 'Text' not in data.columns or 'Classification' not in data.columns:\n",
    "    raise ValueError(\"Required columns ('Text' and 'Classification') not found in the dataset.\")\n",
    "\n",
    "# Load the previously saved TF-IDF vectorizer\n",
    "vectorizer = joblib.load(r\"model\\tfidf_vectorizer.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the texts in the dataset\n",
    "X = vectorizer.transform(data['Text'])\n",
    "y = data['Classification']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Multinomial Naive Bayes model\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "y_pred = mnb.predict(X_test)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save the trained MNB model (optional)\n",
    "joblib.dump(mnb, \"mnb_model.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "report = classification_report(y_test, y_pred, output_dict=True, target_names=le.classes_)\n",
    "\n",
    "# Create DataFrame from the report and only include needed rows and columns\n",
    "df_report = pd.DataFrame(report).transpose()\n",
    "df_report = df_report[['precision', 'recall', 'f1-score', 'support']]\n",
    "df_report = df_report.loc[le.classes_.tolist() + ['accuracy', 'macro avg', 'weighted avg'], :]\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "df_report.to_csv('evaluation_MNB.csv', index_label='class')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv('cleaned data.csv')\n",
    "texts = data['Text'].values\n",
    "labels = data['Classification'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "with open('tokenizer.json', 'r') as file:\n",
    "    json_string = json.load(file)\n",
    "    tokenizer = tokenizer_from_json(json_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and pad sequences\n",
    "maxlen = 250\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train RF classifier\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "# Convert evaluation metrics to DataFrame and save as CSV\n",
    "df_report = pd.DataFrame(report).transpose()\n",
    "df_report.to_csv('rf_evaluation.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'Text' and 'Classification' columns exist\n",
    "if 'Text' not in data.columns or 'Classification' not in data.columns:\n",
    "    raise ValueError(\"The CSV must contain 'Text' and 'Classification' columns.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "with open('tokenizer.json', 'r') as file:\n",
    "    tokenizer_data = json.load(file)\n",
    "    tokenizer = tokenizer_from_json(tokenizer_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the text data into sequences using the tokenizer\n",
    "X = tokenizer.texts_to_sequences(data['Text'])\n",
    "X = pad_sequences(X)\n",
    "\n",
    "# Convert the classification labels into integers\n",
    "y = pd.factorize(data['Classification'])[0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000)  # Increasing max_iter for better convergence\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "# Save the evaluation metrics to a CSV file\n",
    "df_report = pd.DataFrame(report).transpose()\n",
    "df_report.to_csv('evaluation_LR.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.  CNN Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "max_features = len(tokenizer.word_index) + 1\n",
    "maxlen = 250\n",
    "sequences = tokenizer.texts_to_sequences(data['Text'])\n",
    "data_seq = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "# Convert labels to categorical format\n",
    "labels = pd.get_dummies(data['Classification']).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_seq, labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CNN model\n",
    "embedding_dim = 128\n",
    "filters = 128\n",
    "kernel_size = 6\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(max_features, embedding_dim, input_length=maxlen),\n",
    "    Conv1D(filters, kernel_size, activation='relu'),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(labels.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test).argmax(axis=1)\n",
    "y_true = y_test.argmax(axis=1)\n",
    "\n",
    "report = classification_report(y_true, y_pred, output_dict=True)\n",
    "\n",
    "# Save evaluation metrics in the desired format\n",
    "df_report = pd.DataFrame({\n",
    "    'precision': report['weighted avg']['precision'],\n",
    "    'recall': report['weighted avg']['recall'],\n",
    "    'f1-score': report['weighted avg']['f1-score'],\n",
    "    'support': report['weighted avg']['support']\n",
    "}, index=[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
